{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf\n",
    "\n",
    "DATA_URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult'\n",
    "TRAINING_FILE = 'adult.data'\n",
    "TRAINING_URL = '%s/%s' % (DATA_URL, TRAINING_FILE)\n",
    "EVAL_FILE = 'adult.test'\n",
    "EVAL_URL = '%s/%s' % (DATA_URL, EVAL_FILE)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\n",
    "    '--data_dir', type=str, default='./',\n",
    "    help='Directory to download census data')\n",
    "\n",
    "\n",
    "def _download_and_clean_file(filename, url):\n",
    "  \"\"\"Downloads data from url, and makes changes to match the CSV format.\"\"\"\n",
    "  temp_file, _ = urllib.request.urlretrieve(url)\n",
    "  with tf.gfile.Open(temp_file, 'r') as temp_eval_file:\n",
    "    with tf.gfile.Open(filename, 'w') as eval_file:\n",
    "      for line in temp_eval_file:\n",
    "        line = line.strip()\n",
    "        line = line.replace(', ', ',')\n",
    "        if not line or ',' not in line:\n",
    "          continue\n",
    "        if line[-1] == '.':\n",
    "          line = line[:-1]\n",
    "        line += '\\n'\n",
    "        eval_file.write(line)\n",
    "  tf.gfile.Remove(temp_file)\n",
    "\n",
    "\n",
    "def main(_):\n",
    "  if not tf.gfile.Exists(FLAGS.data_dir):\n",
    "    tf.gfile.MkDir(FLAGS.data_dir)\n",
    "\n",
    "  training_file_path = os.path.join(FLAGS.data_dir, TRAINING_FILE)\n",
    "  _download_and_clean_file(training_file_path, TRAINING_URL)\n",
    "\n",
    "  eval_file_path = os.path.join(FLAGS.data_dir, EVAL_FILE)\n",
    "  _download_and_clean_file(eval_file_path, EVAL_URL)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  FLAGS, unparsed = parser.parse_known_args()\n",
    "  tf.app.run(argv=[sys.argv[0]] + unparsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wide component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1028 14:53:33.755602  7056 estimator.py:1790] Using default config.\n",
      "I1028 14:53:33.757597  7056 estimator.py:209] Using config: {'_model_dir': './model/wide_component', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000026AD8B54668>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "I1028 14:53:33.812449  7056 estimator.py:1145] Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train .....\n",
      "Parsing ./data/adult.data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1028 14:53:34.643213  7056 estimator.py:1147] Done calling model_fn.\n",
      "I1028 14:53:34.644210  7056 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
      "I1028 14:53:34.792810  7056 monitored_session.py:240] Graph was finalized.\n",
      "I1028 14:53:34.796800  7056 saver.py:1280] Restoring parameters from ./model/wide_component\\model.ckpt-192\n",
      "I1028 14:53:34.900092  7056 session_manager.py:500] Running local_init_op.\n",
      "I1028 14:53:34.924055  7056 session_manager.py:502] Done running local_init_op.\n",
      "I1028 14:53:35.897060  7056 basic_session_run_hooks.py:606] Saving checkpoints for 192 into ./model/wide_component\\model.ckpt.\n",
      "I1028 14:53:37.688073  7056 basic_session_run_hooks.py:262] loss = 190.58664, step = 193\n",
      "I1028 14:53:38.245586  7056 basic_session_run_hooks.py:606] Saving checkpoints for 256 into ./model/wide_component\\model.ckpt.\n",
      "I1028 14:53:39.065700  7056 estimator.py:368] Loss for final step: 89.07118.\n",
      "I1028 14:53:39.189360  7056 estimator.py:1145] Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing ./data/adult.data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1028 14:53:40.140820  7056 metrics_impl.py:804] Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "W1028 14:53:40.158750  7056 metrics_impl.py:804] Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "I1028 14:53:40.175730  7056 estimator.py:1147] Done calling model_fn.\n",
      "I1028 14:53:40.190690  7056 evaluation.py:255] Starting evaluation at 2019-10-28T14:53:40Z\n",
      "I1028 14:53:40.286406  7056 monitored_session.py:240] Graph was finalized.\n",
      "I1028 14:53:40.291393  7056 saver.py:1280] Restoring parameters from ./model/wide_component\\model.ckpt-256\n",
      "I1028 14:53:40.379157  7056 session_manager.py:500] Running local_init_op.\n",
      "I1028 14:53:40.417055  7056 session_manager.py:502] Done running local_init_op.\n",
      "I1028 14:53:41.234852  7056 evaluation.py:275] Finished evaluation at 2019-10-28-14:53:41\n",
      "I1028 14:53:41.235852  7056 estimator.py:2039] Saving dict for global step 256: accuracy = 0.8407911, accuracy_baseline = 0.75919044, auc = 0.8906963, auc_precision_recall = 0.71676695, average_loss = 0.34834406, global_step = 256, label/mean = 0.24080956, loss = 177.22548, precision = 0.72814703, prediction/mean = 0.24617735, recall = 0.54074734\n",
      "I1028 14:53:41.296309  7056 estimator.py:2099] Saving 'checkpoint_path' summary for global step 256: ./model/wide_component\\model.ckpt-256\n",
      "I1028 14:53:41.440346  7056 estimator.py:1145] Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy            : 0.8408\n",
      "accuracy_baseline   : 0.7592\n",
      "auc                 : 0.8907\n",
      "auc_precision_recall: 0.7168\n",
      "average_loss        : 0.3483\n",
      "global_step         : 256.0000\n",
      "label/mean          : 0.2408\n",
      "loss                : 177.2255\n",
      "precision           : 0.7281\n",
      "prediction/mean     : 0.2462\n",
      "recall              : 0.5407\n",
      "Predict .....\n",
      "Parsing ./data/adult.test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1028 14:53:41.928039  7056 estimator.py:1147] Done calling model_fn.\n",
      "I1028 14:53:42.117522  7056 monitored_session.py:240] Graph was finalized.\n",
      "I1028 14:53:42.121500  7056 saver.py:1280] Restoring parameters from ./model/wide_component\\model.ckpt-256\n",
      "I1028 14:53:42.183352  7056 session_manager.py:500] Running local_init_op.\n",
      "I1028 14:53:42.201302  7056 session_manager.py:502] Done running local_init_op.\n",
      "I1028 14:53:42.405745  7056 estimator.py:1145] Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logits': array([-4.784633], dtype=float32), 'logistic': array([0.00828793], dtype=float32), 'probabilities': array([0.9917121 , 0.00828793], dtype=float32), 'class_ids': array([0], dtype=int64), 'classes': array([b'0'], dtype=object), 'all_class_ids': array([0, 1]), 'all_classes': array([b'0', b'1'], dtype=object)}\n",
      "test .....\n",
      "Parsing ./data/adult.test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1028 14:53:43.346195  7056 metrics_impl.py:804] Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "W1028 14:53:43.362178  7056 metrics_impl.py:804] Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "I1028 14:53:43.380119  7056 estimator.py:1147] Done calling model_fn.\n",
      "I1028 14:53:43.397086  7056 evaluation.py:255] Starting evaluation at 2019-10-28T14:53:43Z\n",
      "I1028 14:53:43.488837  7056 monitored_session.py:240] Graph was finalized.\n",
      "I1028 14:53:43.492799  7056 saver.py:1280] Restoring parameters from ./model/wide_component\\model.ckpt-256\n",
      "I1028 14:53:43.582591  7056 session_manager.py:500] Running local_init_op.\n",
      "I1028 14:53:43.619492  7056 session_manager.py:502] Done running local_init_op.\n",
      "I1028 14:53:44.248796  7056 evaluation.py:275] Finished evaluation at 2019-10-28-14:53:44\n",
      "I1028 14:53:44.249793  7056 estimator.py:2039] Saving dict for global step 256: accuracy = 0.83717215, accuracy_baseline = 0.76377374, auc = 0.8846975, auc_precision_recall = 0.69667923, average_loss = 0.3525057, global_step = 256, label/mean = 0.23622628, loss = 179.3483, precision = 0.7069622, prediction/mean = 0.24388611, recall = 0.53068125\n",
      "I1028 14:53:44.304854  7056 estimator.py:2099] Saving 'checkpoint_path' summary for global step 256: ./model/wide_component\\model.ckpt-256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy            : 0.8372\n",
      "accuracy_baseline   : 0.7638\n",
      "auc                 : 0.8847\n",
      "auc_precision_recall: 0.6967\n",
      "average_loss        : 0.3525\n",
      "global_step         : 256.0000\n",
      "label/mean          : 0.2362\n",
      "loss                : 179.3483\n",
      "precision           : 0.7070\n",
      "prediction/mean     : 0.2439\n",
      "recall              : 0.5307\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "_CSV_COLUMNS = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "    'marital_status', 'occupation', 'relationship', 'race', 'gender',\n",
    "    'capital_gain', 'capital_loss', 'hours_per_week', 'native_country',\n",
    "    'income_bracket'\n",
    "]\n",
    "\n",
    "_CSV_COLUMN_DEFAULTS = [[0], [''], [0], [''], [0], [''], [''], [''], [''], [''],\n",
    "                        [0], [0], [0], [''], ['']]\n",
    "\n",
    "_NUM_EXAMPLES = {\n",
    "    'train': 32561,\n",
    "    'validation': 16281,\n",
    "}\n",
    "\n",
    "\n",
    "# 1. Read the Census Data\n",
    "\n",
    "# 2. Converting Data into Tensors\n",
    "def input_fn(data_file, num_epochs, shuffle, batch_size):\n",
    "    \"\"\"为Estimator创建一个input function\"\"\"\n",
    "    assert tf.gfile.Exists(data_file), \"{0} not found.\".format(data_file)\n",
    "\n",
    "    def parse_csv(line):\n",
    "        print(\"Parsing\", data_file)\n",
    "        # tf.decode_csv会把csv文件转换成很a list of Tensor,一列一个。record_defaults用于指明每一列的缺失值用什么填充\n",
    "        columns = tf.decode_csv(line, record_defaults=_CSV_COLUMN_DEFAULTS)\n",
    "        features = dict(zip(_CSV_COLUMNS, columns))\n",
    "        labels = features.pop('income_bracket')\n",
    "        return features, tf.equal(labels, '>50K') # tf.equal(x, y) 返回一个bool类型Tensor， 表示x == y, element-wise\n",
    "\n",
    "    dataset = tf.data.TextLineDataset(data_file) \\\n",
    "                .map(parse_csv, num_parallel_calls=5)\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=_NUM_EXAMPLES['train'] + _NUM_EXAMPLES['validation'])\n",
    "\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    batch_features, batch_labels = iterator.get_next()\n",
    "    return batch_features, batch_labels\n",
    "\n",
    "# 3. Select and Engineer Features for Model\n",
    "\n",
    "## 3.1 Base Categorical Feature Columns\n",
    "# 如果我们知道所有的取值，并且取值不是很多\n",
    "relationship = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    'relationship', [\n",
    "        'Husband', 'Not-in-family', 'Wife', 'Own-child', 'Unmarried',\n",
    "        'Other-relative'\n",
    "    ]\n",
    ")\n",
    "# 如果不知道有多少取值\n",
    "occupation = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "    'occupation', hash_bucket_size=1000\n",
    ")\n",
    "\n",
    "education = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    'education', [\n",
    "        'Bachelors', 'HS-grad', '11th', 'Masters', '9th', 'Some-college',\n",
    "        'Assoc-acdm', 'Assoc-voc', '7th-8th', 'Doctorate', 'Prof-school',\n",
    "        '5th-6th', '10th', '1st-4th', 'Preschool', '12th'\n",
    "    ]\n",
    ")\n",
    "\n",
    "marital_status = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "'marital_status', [\n",
    "        'Married-civ-spouse', 'Divorced', 'Married-spouse-absent',\n",
    "        'Never-married', 'Separated', 'Married-AF-spouse', 'Widowed']\n",
    ")\n",
    "\n",
    "\n",
    "workclass = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    'workclass', [\n",
    "        'Self-emp-not-inc', 'Private', 'State-gov', 'Federal-gov',\n",
    "        'Local-gov', '?', 'Self-emp-inc', 'Without-pay', 'Never-worked'])\n",
    "\n",
    "\n",
    "# 3.2 Base Continuous Feature Columns\n",
    "age = tf.feature_column.numeric_column('age')\n",
    "education_num = tf.feature_column.numeric_column('education_num')\n",
    "capital_gain = tf.feature_column.numeric_column('capital_gain')\n",
    "capital_loss = tf.feature_column.numeric_column('capital_loss')\n",
    "hours_per_week = tf.feature_column.numeric_column('hours_per_week')\n",
    "\n",
    "#\n",
    "\"\"\"\n",
    "Sometimes the relationship between a continuous feature and the label is not linear. As a hypothetical example, \n",
    "a person's income may grow with age in the early stage of one's career, then the growth may slow at some point, \n",
    "and finally the income decreases after retirement. \n",
    "In this scenario, using the raw age as a real-valued feature column might not be a good choice because the model \n",
    "can only learn one of the three cases:\n",
    "\"\"\"    \n",
    "    \n",
    "# 3.2.1 连续特征离散化\n",
    "# 之所以这么做是因为：有些时候连续特征和label之间不是线性的关系。可能刚开始是正的线性关系，后面又变成了负的线性关系，\n",
    "# 这样一个折线的关系整体来看就不再是线性关系。\n",
    "# bucketization 装桶\n",
    "# 10个边界，11个桶\n",
    "age_buckets = tf.feature_column.bucketized_column(\n",
    "    age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
    "\n",
    "# 3.3 组合特征/交叉特征\n",
    "education_x_occupation = tf.feature_column.crossed_column(\n",
    "    ['education', 'occupation'], hash_bucket_size=1000)\n",
    "\n",
    "age_buckets_x_education_x_occupation = tf.feature_column.crossed_column(\n",
    "    [age_buckets, 'education', 'occupation'], hash_bucket_size=1000\n",
    ")\n",
    "\n",
    "\n",
    "# 4. 模型\n",
    "\"\"\"\n",
    "之前的特征：\n",
    "1. CategoricalColumn\n",
    "2. NumericalColumn\n",
    "3. BucketizedColumn\n",
    "4. CrossedColumn\n",
    "这些特征都是FeatureColumn的子类，可以放到一起\n",
    "\"\"\"\n",
    "base_columns = [\n",
    "    education, marital_status, relationship, workclass, occupation,\n",
    "    age_buckets,\n",
    "]\n",
    "\n",
    "crossed_column = [\n",
    "    tf.feature_column.crossed_column(\n",
    "        ['education', 'occupation'], hash_bucket_size=1000\n",
    "    ),\n",
    "    tf.feature_column.crossed_column(\n",
    "        [age_buckets, 'education', 'occupation'], hash_bucket_size=1000\n",
    "    )\n",
    "]\n",
    "\n",
    "model_dir = \"./model/wide_component\"\n",
    "#model = tf.estimator.LinearClassifier(\n",
    "#    model_dir=model_dir, feature_columns=base_columns + crossed_column\n",
    "#)\n",
    "\n",
    "# 6. 正则化\n",
    "model = tf.estimator.LinearClassifier(\n",
    "    feature_columns=base_columns + crossed_column, model_dir=model_dir,\n",
    "    optimizer=tf.train.FtrlOptimizer(\n",
    "        learning_rate=0.1,\n",
    "        l1_regularization_strength=1.0,\n",
    "        l2_regularization_strength=1.0\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "train_file = './data/adult.data'\n",
    "val_file = './data/adult.data'\n",
    "test_file = './data/adult.test'\n",
    "\n",
    "# 5. Train & Evaluate & Predict\n",
    "print(\"Train .....\")\n",
    "model.train(input_fn=lambda: input_fn(data_file=train_file, num_epochs=1, shuffle=True, batch_size=512))\n",
    "results = model.evaluate(input_fn=lambda: input_fn(val_file, 1, False, 512))\n",
    "for key in sorted(results):\n",
    "    print(\"{0:20}: {1:.4f}\".format(key, results[key]))\n",
    "\n",
    "print(\"Predict .....\")\n",
    "pred_iter = model.predict(input_fn=lambda: input_fn(test_file, 1, False, 1))\n",
    "for pred in pred_iter:\n",
    "    print(pred)\n",
    "    break #太多了，只打印一条\n",
    "\n",
    "print(\"test .....\")\n",
    "test_results = model.evaluate(input_fn=lambda: input_fn(test_file, 1, False, 512))\n",
    "for key in sorted(test_results):\n",
    "    print(\"{0:20}: {1:.4f}\".format(key, test_results[key]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     print(tf.VERSION)\n",
    "#     data_file = './data/adult.data'\n",
    "#     next_batch = input_fn(data_file, num_epochs=1, shuffle=True, batch_size=5)\n",
    "#     with tf.Session() as sess:\n",
    "#         first_batch = sess.run(next_batch)\n",
    "#         print(first_batch[0])\n",
    "#         print(first_batch[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  wide & deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1025 10:51:09.071384  7056 estimator.py:1790] Using default config.\n",
      "I1025 10:51:09.073353  7056 estimator.py:209] Using config: {'_model_dir': './model/wide_deep', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000026AD13A8EF0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "I1025 10:51:09.125210  7056 estimator.py:1145] Calling model_fn.\n",
      "W1025 10:51:09.127235  7056 deprecation.py:506] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1025 10:51:09.286806  7056 deprecation.py:323] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py:3038: HashedCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing ./data/adult.data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1025 10:51:09.443383  7056 deprecation.py:323] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py:4207: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "W1025 10:51:09.444378  7056 deprecation.py:323] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py:4262: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "W1025 10:51:10.585306  7056 deprecation.py:506] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "I1025 10:51:10.869541  7056 estimator.py:1147] Done calling model_fn.\n",
      "I1025 10:51:10.869541  7056 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
      "I1025 10:51:11.598765  7056 monitored_session.py:240] Graph was finalized.\n",
      "I1025 10:51:11.731416  7056 session_manager.py:500] Running local_init_op.\n",
      "I1025 10:51:11.766295  7056 session_manager.py:502] Done running local_init_op.\n",
      "I1025 10:51:12.736651  7056 basic_session_run_hooks.py:606] Saving checkpoints for 0 into ./model/wide_deep\\model.ckpt.\n",
      "I1025 10:51:15.015208  7056 basic_session_run_hooks.py:262] loss = 1404.1313, step = 1\n",
      "I1025 10:51:15.523838  7056 basic_session_run_hooks.py:692] global_step/sec: 196.221\n",
      "I1025 10:51:15.525834  7056 basic_session_run_hooks.py:260] loss = 21.052208, step = 101 (0.510 sec)\n",
      "I1025 10:51:15.699367  7056 basic_session_run_hooks.py:692] global_step/sec: 569.709\n",
      "I1025 10:51:15.700364  7056 basic_session_run_hooks.py:260] loss = 17.472694, step = 201 (0.176 sec)\n",
      "I1025 10:51:15.875891  7056 basic_session_run_hooks.py:692] global_step/sec: 566.495\n",
      "I1025 10:51:15.877886  7056 basic_session_run_hooks.py:260] loss = 24.373453, step = 301 (0.178 sec)\n",
      "I1025 10:51:16.052416  7056 basic_session_run_hooks.py:692] global_step/sec: 566.492\n",
      "I1025 10:51:16.053413  7056 basic_session_run_hooks.py:260] loss = 11.806082, step = 401 (0.176 sec)\n",
      "I1025 10:51:16.170099  7056 basic_session_run_hooks.py:692] global_step/sec: 849.736\n",
      "I1025 10:51:16.171096  7056 basic_session_run_hooks.py:260] loss = 19.738657, step = 501 (0.118 sec)\n",
      "I1025 10:51:16.284790  7056 basic_session_run_hooks.py:692] global_step/sec: 871.908\n",
      "I1025 10:51:16.285787  7056 basic_session_run_hooks.py:260] loss = 46.602455, step = 601 (0.115 sec)\n",
      "I1025 10:51:16.404468  7056 basic_session_run_hooks.py:692] global_step/sec: 835.577\n",
      "I1025 10:51:16.405465  7056 basic_session_run_hooks.py:260] loss = 12.999855, step = 701 (0.120 sec)\n",
      "I1025 10:51:16.521154  7056 basic_session_run_hooks.py:692] global_step/sec: 857\n",
      "I1025 10:51:16.522151  7056 basic_session_run_hooks.py:260] loss = 15.428493, step = 801 (0.117 sec)\n",
      "I1025 10:51:16.643824  7056 basic_session_run_hooks.py:692] global_step/sec: 815.197\n",
      "I1025 10:51:16.644821  7056 basic_session_run_hooks.py:260] loss = 19.890327, step = 901 (0.123 sec)\n",
      "I1025 10:51:16.760509  7056 basic_session_run_hooks.py:692] global_step/sec: 857.004\n",
      "I1025 10:51:16.761507  7056 basic_session_run_hooks.py:260] loss = 13.902189, step = 1001 (0.117 sec)\n",
      "I1025 10:51:16.872208  7056 basic_session_run_hooks.py:692] global_step/sec: 895.264\n",
      "I1025 10:51:16.874203  7056 basic_session_run_hooks.py:260] loss = 18.641655, step = 1101 (0.113 sec)\n",
      "I1025 10:51:16.988894  7056 basic_session_run_hooks.py:692] global_step/sec: 857\n",
      "I1025 10:51:16.989892  7056 basic_session_run_hooks.py:260] loss = 17.462801, step = 1201 (0.116 sec)\n",
      "I1025 10:51:17.102588  7056 basic_session_run_hooks.py:692] global_step/sec: 879.556\n",
      "I1025 10:51:17.103585  7056 basic_session_run_hooks.py:260] loss = 15.650592, step = 1301 (0.114 sec)\n",
      "I1025 10:51:17.217279  7056 basic_session_run_hooks.py:692] global_step/sec: 871.906\n",
      "I1025 10:51:17.219274  7056 basic_session_run_hooks.py:260] loss = 17.338295, step = 1401 (0.116 sec)\n",
      "I1025 10:51:17.333965  7056 basic_session_run_hooks.py:692] global_step/sec: 857.002\n",
      "I1025 10:51:17.334963  7056 basic_session_run_hooks.py:260] loss = 12.137505, step = 1501 (0.116 sec)\n",
      "I1025 10:51:17.452646  7056 basic_session_run_hooks.py:692] global_step/sec: 842.593\n",
      "I1025 10:51:17.453644  7056 basic_session_run_hooks.py:260] loss = 13.522712, step = 1601 (0.119 sec)\n",
      "I1025 10:51:17.525451  7056 basic_session_run_hooks.py:606] Saving checkpoints for 1629 into ./model/wide_deep\\model.ckpt.\n",
      "I1025 10:51:18.483810  7056 estimator.py:368] Loss for final step: 0.220463.\n",
      "I1025 10:51:18.528683  7056 estimator.py:1145] Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing ./data/adult.test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1025 10:51:19.783305  7056 metrics_impl.py:804] Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "W1025 10:51:19.800260  7056 metrics_impl.py:804] Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "I1025 10:51:19.817216  7056 estimator.py:1147] Done calling model_fn.\n",
      "I1025 10:51:19.833145  7056 evaluation.py:255] Starting evaluation at 2019-10-25T10:51:19Z\n",
      "I1025 10:51:19.950846  7056 monitored_session.py:240] Graph was finalized.\n",
      "I1025 10:51:19.955814  7056 saver.py:1280] Restoring parameters from ./model/wide_deep\\model.ckpt-1629\n",
      "I1025 10:51:20.066543  7056 session_manager.py:500] Running local_init_op.\n",
      "I1025 10:51:20.112393  7056 session_manager.py:502] Done running local_init_op.\n",
      "I1025 10:51:21.005022  7056 evaluation.py:275] Finished evaluation at 2019-10-25-10:51:21\n",
      "I1025 10:51:21.006992  7056 estimator.py:2039] Saving dict for global step 1629: accuracy = 0.83649653, accuracy_baseline = 0.76377374, auc = 0.8805206, auc_precision_recall = 0.71628904, average_loss = 0.3705278, global_step = 1629, label/mean = 0.23622628, loss = 14.785693, precision = 0.7396761, prediction/mean = 0.2614891, recall = 0.475039\n",
      "I1025 10:51:21.562915  7056 estimator.py:2099] Saving 'checkpoint_path' summary for global step 1629: ./model/wide_deep\\model.ckpt-1629\n",
      "I1025 10:51:21.689639  7056 estimator.py:1145] Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at epoch 2\n",
      "------------------------------\n",
      "accuracy            : 0.8365\n",
      "accuracy_baseline   : 0.7638\n",
      "auc                 : 0.8805\n",
      "auc_precision_recall: 0.7163\n",
      "average_loss        : 0.3705\n",
      "global_step         : 1629.0000\n",
      "label/mean          : 0.2362\n",
      "loss                : 14.7857\n",
      "precision           : 0.7397\n",
      "prediction/mean     : 0.2615\n",
      "recall              : 0.4750\n",
      "Parsing ./data/adult.data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1025 10:51:22.906556  7056 estimator.py:1147] Done calling model_fn.\n",
      "I1025 10:51:22.907536  7056 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
      "I1025 10:51:23.114007  7056 monitored_session.py:240] Graph was finalized.\n",
      "I1025 10:51:23.119964  7056 saver.py:1280] Restoring parameters from ./model/wide_deep\\model.ckpt-1629\n",
      "I1025 10:51:23.255624  7056 session_manager.py:500] Running local_init_op.\n",
      "I1025 10:51:23.287539  7056 session_manager.py:502] Done running local_init_op.\n",
      "I1025 10:51:24.188347  7056 basic_session_run_hooks.py:606] Saving checkpoints for 1629 into ./model/wide_deep\\model.ckpt.\n",
      "I1025 10:51:26.377677  7056 basic_session_run_hooks.py:262] loss = 14.380152, step = 1630\n",
      "I1025 10:51:26.893378  7056 basic_session_run_hooks.py:692] global_step/sec: 193.537\n",
      "I1025 10:51:26.894375  7056 basic_session_run_hooks.py:260] loss = 15.490909, step = 1730 (0.517 sec)\n",
      "I1025 10:51:27.073893  7056 basic_session_run_hooks.py:692] global_step/sec: 557.05\n",
      "I1025 10:51:27.074891  7056 basic_session_run_hooks.py:260] loss = 16.224274, step = 1830 (0.181 sec)\n",
      "I1025 10:51:27.251414  7056 basic_session_run_hooks.py:692] global_step/sec: 560.163\n",
      "I1025 10:51:27.252413  7056 basic_session_run_hooks.py:260] loss = 15.102732, step = 1930 (0.178 sec)\n",
      "I1025 10:51:27.431928  7056 basic_session_run_hooks.py:692] global_step/sec: 553.974\n",
      "I1025 10:51:27.432925  7056 basic_session_run_hooks.py:260] loss = 13.502743, step = 2030 (0.181 sec)\n",
      "I1025 10:51:27.552603  7056 basic_session_run_hooks.py:692] global_step/sec: 828.671\n",
      "I1025 10:51:27.553601  7056 basic_session_run_hooks.py:260] loss = 13.518603, step = 2130 (0.121 sec)\n",
      "I1025 10:51:27.667294  7056 basic_session_run_hooks.py:692] global_step/sec: 871.908\n",
      "I1025 10:51:27.668292  7056 basic_session_run_hooks.py:260] loss = 13.688498, step = 2230 (0.115 sec)\n",
      "I1025 10:51:27.783981  7056 basic_session_run_hooks.py:692] global_step/sec: 856.997\n",
      "I1025 10:51:27.784978  7056 basic_session_run_hooks.py:260] loss = 13.630005, step = 2330 (0.117 sec)\n",
      "I1025 10:51:27.896678  7056 basic_session_run_hooks.py:692] global_step/sec: 887.336\n",
      "I1025 10:51:27.897674  7056 basic_session_run_hooks.py:260] loss = 9.679502, step = 2430 (0.113 sec)\n",
      "I1025 10:51:28.012366  7056 basic_session_run_hooks.py:692] global_step/sec: 864.395\n",
      "I1025 10:51:28.013363  7056 basic_session_run_hooks.py:260] loss = 10.996582, step = 2530 (0.116 sec)\n",
      "I1025 10:51:28.128054  7056 basic_session_run_hooks.py:692] global_step/sec: 864.388\n",
      "I1025 10:51:28.129052  7056 basic_session_run_hooks.py:260] loss = 16.72432, step = 2630 (0.116 sec)\n",
      "I1025 10:51:28.242745  7056 basic_session_run_hooks.py:692] global_step/sec: 871.908\n",
      "I1025 10:51:28.243742  7056 basic_session_run_hooks.py:260] loss = 14.943228, step = 2730 (0.115 sec)\n",
      "I1025 10:51:28.359431  7056 basic_session_run_hooks.py:692] global_step/sec: 857.002\n",
      "I1025 10:51:28.360428  7056 basic_session_run_hooks.py:260] loss = 15.436523, step = 2830 (0.117 sec)\n",
      "I1025 10:51:28.479109  7056 basic_session_run_hooks.py:692] global_step/sec: 835.577\n",
      "I1025 10:51:28.481104  7056 basic_session_run_hooks.py:260] loss = 14.420729, step = 2930 (0.121 sec)\n",
      "I1025 10:51:28.600782  7056 basic_session_run_hooks.py:692] global_step/sec: 821.878\n",
      "I1025 10:51:28.601779  7056 basic_session_run_hooks.py:260] loss = 13.814746, step = 3030 (0.121 sec)\n",
      "I1025 10:51:28.720459  7056 basic_session_run_hooks.py:692] global_step/sec: 835.579\n",
      "I1025 10:51:28.721457  7056 basic_session_run_hooks.py:260] loss = 10.505714, step = 3130 (0.120 sec)\n",
      "I1025 10:51:28.838142  7056 basic_session_run_hooks.py:692] global_step/sec: 849.739\n",
      "I1025 10:51:28.839140  7056 basic_session_run_hooks.py:260] loss = 10.372901, step = 3230 (0.118 sec)\n",
      "I1025 10:51:28.911944  7056 basic_session_run_hooks.py:606] Saving checkpoints for 3258 into ./model/wide_deep\\model.ckpt.\n",
      "I1025 10:51:29.841112  7056 estimator.py:368] Loss for final step: 0.6272555.\n",
      "I1025 10:51:29.889978  7056 estimator.py:1145] Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing ./data/adult.test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1025 10:51:31.104682  7056 metrics_impl.py:804] Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "W1025 10:51:31.121664  7056 metrics_impl.py:804] Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "I1025 10:51:31.138617  7056 estimator.py:1147] Done calling model_fn.\n",
      "I1025 10:51:31.216407  7056 evaluation.py:255] Starting evaluation at 2019-10-25T10:51:31Z\n",
      "I1025 10:51:31.338073  7056 monitored_session.py:240] Graph was finalized.\n",
      "I1025 10:51:31.343039  7056 saver.py:1280] Restoring parameters from ./model/wide_deep\\model.ckpt-3258\n",
      "I1025 10:51:31.452770  7056 session_manager.py:500] Running local_init_op.\n",
      "I1025 10:51:31.498630  7056 session_manager.py:502] Done running local_init_op.\n",
      "I1025 10:51:32.404186  7056 evaluation.py:275] Finished evaluation at 2019-10-25-10:51:32\n",
      "I1025 10:51:32.405184  7056 estimator.py:2039] Saving dict for global step 3258: accuracy = 0.84828943, accuracy_baseline = 0.76377374, auc = 0.89306474, auc_precision_recall = 0.7446954, average_loss = 0.34513745, global_step = 3258, label/mean = 0.23622628, loss = 13.772507, precision = 0.7576779, prediction/mean = 0.25185892, recall = 0.52600104\n",
      "I1025 10:51:32.464279  7056 estimator.py:2099] Saving 'checkpoint_path' summary for global step 3258: ./model/wide_deep\\model.ckpt-3258\n",
      "I1025 10:51:32.609018  7056 estimator.py:1145] Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at epoch 4\n",
      "------------------------------\n",
      "accuracy            : 0.8483\n",
      "accuracy_baseline   : 0.7638\n",
      "auc                 : 0.8931\n",
      "auc_precision_recall: 0.7447\n",
      "average_loss        : 0.3451\n",
      "global_step         : 3258.0000\n",
      "label/mean          : 0.2362\n",
      "loss                : 13.7725\n",
      "precision           : 0.7577\n",
      "prediction/mean     : 0.2519\n",
      "recall              : 0.5260\n",
      "Parsing ./data/adult.data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1025 10:51:33.781861  7056 estimator.py:1147] Done calling model_fn.\n",
      "I1025 10:51:33.782855  7056 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
      "I1025 10:51:34.048143  7056 monitored_session.py:240] Graph was finalized.\n",
      "I1025 10:51:34.054101  7056 saver.py:1280] Restoring parameters from ./model/wide_deep\\model.ckpt-3258\n",
      "I1025 10:51:34.186771  7056 session_manager.py:500] Running local_init_op.\n",
      "I1025 10:51:34.219655  7056 session_manager.py:502] Done running local_init_op.\n",
      "I1025 10:51:35.190168  7056 basic_session_run_hooks.py:606] Saving checkpoints for 3258 into ./model/wide_deep\\model.ckpt.\n",
      "I1025 10:51:37.261632  7056 basic_session_run_hooks.py:262] loss = 11.329519, step = 3259\n",
      "I1025 10:51:37.772257  7056 basic_session_run_hooks.py:692] global_step/sec: 195.838\n",
      "I1025 10:51:37.773254  7056 basic_session_run_hooks.py:260] loss = 12.077276, step = 3359 (0.512 sec)\n",
      "I1025 10:51:37.954766  7056 basic_session_run_hooks.py:692] global_step/sec: 547.92\n",
      "I1025 10:51:37.955763  7056 basic_session_run_hooks.py:260] loss = 13.42743, step = 3459 (0.183 sec)\n",
      "I1025 10:51:38.133285  7056 basic_session_run_hooks.py:692] global_step/sec: 560.161\n",
      "I1025 10:51:38.134282  7056 basic_session_run_hooks.py:260] loss = 12.664218, step = 3559 (0.179 sec)\n",
      "I1025 10:51:38.313800  7056 basic_session_run_hooks.py:692] global_step/sec: 553.973\n",
      "I1025 10:51:38.314796  7056 basic_session_run_hooks.py:260] loss = 14.463655, step = 3659 (0.181 sec)\n",
      "I1025 10:51:38.434474  7056 basic_session_run_hooks.py:692] global_step/sec: 828.675\n",
      "I1025 10:51:38.435471  7056 basic_session_run_hooks.py:260] loss = 7.788604, step = 3759 (0.121 sec)\n",
      "I1025 10:51:38.549165  7056 basic_session_run_hooks.py:692] global_step/sec: 871.906\n",
      "I1025 10:51:38.550163  7056 basic_session_run_hooks.py:260] loss = 14.829592, step = 3859 (0.115 sec)\n",
      "I1025 10:51:38.663857  7056 basic_session_run_hooks.py:692] global_step/sec: 871.906\n",
      "I1025 10:51:38.664854  7056 basic_session_run_hooks.py:260] loss = 11.208252, step = 3959 (0.115 sec)\n",
      "I1025 10:51:38.779545  7056 basic_session_run_hooks.py:692] global_step/sec: 864.39\n",
      "I1025 10:51:38.780542  7056 basic_session_run_hooks.py:260] loss = 9.971224, step = 4059 (0.116 sec)\n",
      "I1025 10:51:38.896231  7056 basic_session_run_hooks.py:692] global_step/sec: 857.002\n",
      "I1025 10:51:38.897228  7056 basic_session_run_hooks.py:260] loss = 14.043493, step = 4159 (0.117 sec)\n",
      "I1025 10:51:39.013914  7056 basic_session_run_hooks.py:692] global_step/sec: 849.739\n",
      "I1025 10:51:39.014912  7056 basic_session_run_hooks.py:260] loss = 15.922236, step = 4259 (0.118 sec)\n",
      "I1025 10:51:39.129603  7056 basic_session_run_hooks.py:692] global_step/sec: 871.902\n",
      "I1025 10:51:39.130601  7056 basic_session_run_hooks.py:260] loss = 11.996294, step = 4359 (0.116 sec)\n",
      "I1025 10:51:39.245292  7056 basic_session_run_hooks.py:692] global_step/sec: 864.395\n",
      "I1025 10:51:39.246289  7056 basic_session_run_hooks.py:260] loss = 10.631051, step = 4459 (0.116 sec)\n",
      "I1025 10:51:39.361977  7056 basic_session_run_hooks.py:692] global_step/sec: 849.739\n",
      "I1025 10:51:39.362974  7056 basic_session_run_hooks.py:260] loss = 12.275616, step = 4559 (0.117 sec)\n",
      "I1025 10:51:39.481655  7056 basic_session_run_hooks.py:692] global_step/sec: 835.575\n",
      "I1025 10:51:39.482652  7056 basic_session_run_hooks.py:260] loss = 7.018407, step = 4659 (0.120 sec)\n",
      "I1025 10:51:39.594352  7056 basic_session_run_hooks.py:692] global_step/sec: 887.338\n",
      "I1025 10:51:39.596346  7056 basic_session_run_hooks.py:260] loss = 14.585466, step = 4759 (0.114 sec)\n",
      "I1025 10:51:39.710040  7056 basic_session_run_hooks.py:692] global_step/sec: 864.391\n",
      "I1025 10:51:39.711037  7056 basic_session_run_hooks.py:260] loss = 15.248853, step = 4859 (0.115 sec)\n",
      "I1025 10:51:39.784838  7056 basic_session_run_hooks.py:606] Saving checkpoints for 4887 into ./model/wide_deep\\model.ckpt.\n",
      "I1025 10:51:40.709344  7056 estimator.py:368] Loss for final step: 0.14392.\n",
      "I1025 10:51:40.764221  7056 estimator.py:1145] Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing ./data/adult.test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1025 10:51:41.933068  7056 metrics_impl.py:804] Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "W1025 10:51:41.950003  7056 metrics_impl.py:804] Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "I1025 10:51:41.967955  7056 estimator.py:1147] Done calling model_fn.\n",
      "I1025 10:51:41.982940  7056 evaluation.py:255] Starting evaluation at 2019-10-25T10:51:41Z\n",
      "I1025 10:51:42.165493  7056 monitored_session.py:240] Graph was finalized.\n",
      "I1025 10:51:42.169459  7056 saver.py:1280] Restoring parameters from ./model/wide_deep\\model.ckpt-4887\n",
      "I1025 10:51:42.278182  7056 session_manager.py:500] Running local_init_op.\n",
      "I1025 10:51:42.321076  7056 session_manager.py:502] Done running local_init_op.\n",
      "I1025 10:51:43.206663  7056 evaluation.py:275] Finished evaluation at 2019-10-25-10:51:43\n",
      "I1025 10:51:43.207660  7056 estimator.py:2039] Saving dict for global step 4887: accuracy = 0.8518519, accuracy_baseline = 0.76377374, auc = 0.8950233, auc_precision_recall = 0.7527445, average_loss = 0.33886144, global_step = 4887, label/mean = 0.23622628, loss = 13.522066, precision = 0.7540751, prediction/mean = 0.25012824, recall = 0.5533021\n",
      "I1025 10:51:43.267762  7056 estimator.py:2099] Saving 'checkpoint_path' summary for global step 4887: ./model/wide_deep\\model.ckpt-4887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at epoch 6\n",
      "------------------------------\n",
      "accuracy            : 0.8519\n",
      "accuracy_baseline   : 0.7638\n",
      "auc                 : 0.8950\n",
      "auc_precision_recall: 0.7527\n",
      "average_loss        : 0.3389\n",
      "global_step         : 4887.0000\n",
      "label/mean          : 0.2362\n",
      "loss                : 13.5221\n",
      "precision           : 0.7541\n",
      "prediction/mean     : 0.2501\n",
      "recall              : 0.5533\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from absl import flags\n",
    "from absl import app\n",
    "\n",
    "\n",
    "# 1. 最基本的特征：\n",
    "\n",
    "# Continuous columns. Wide和Deep组件都会用到。\n",
    "age = tf.feature_column.numeric_column('age')\n",
    "education_num = tf.feature_column.numeric_column('education_num')\n",
    "capital_gain = tf.feature_column.numeric_column('capital_gain')\n",
    "capital_loss = tf.feature_column.numeric_column('capital_loss')\n",
    "hours_per_week = tf.feature_column.numeric_column('hours_per_week')\n",
    "\n",
    "# 离散特征 16dim\n",
    "education = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    'education', [\n",
    "        'Bachelors', 'HS-grad', '11th', 'Masters', '9th', 'Some-college',\n",
    "        'Assoc-acdm', 'Assoc-voc', '7th-8th', 'Doctorate', 'Prof-school',\n",
    "        '5th-6th', '10th', '1st-4th', 'Preschool', '12th'])\n",
    "\n",
    "# 7 dim\n",
    "marital_status = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    'marital_status', [\n",
    "        'Married-civ-spouse', 'Divorced', 'Married-spouse-absent',\n",
    "        'Never-married', 'Separated', 'Married-AF-spouse', 'Widowed'])\n",
    "\n",
    "# 6 dim\n",
    "relationship = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    'relationship', [\n",
    "        'Husband', 'Not-in-family', 'Wife', 'Own-child', 'Unmarried',\n",
    "        'Other-relative'])\n",
    "\n",
    "# 9 dim\n",
    "workclass = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    'workclass', [\n",
    "        'Self-emp-not-inc', 'Private', 'State-gov', 'Federal-gov',\n",
    "        'Local-gov', '?', 'Self-emp-inc', 'Without-pay', 'Never-worked'])\n",
    "\n",
    "# 展示一下这个API\n",
    "occupation = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "    'occupation', hash_bucket_size=1000\n",
    ")\n",
    "\n",
    "# Transformations\n",
    "age_buckets = tf.feature_column.bucketized_column(\n",
    "    age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65]\n",
    ")\n",
    "\n",
    "# 2. The Wide Model: Linear Model with CrossedFeatureColumns\n",
    "\"\"\"\n",
    "The wide model is a linear model with a wide set of *sparse and crossed feature* columns\n",
    "Wide部分用了一个规范化后的连续特征age_buckets，其他的连续特征没有使用\n",
    "\"\"\"\n",
    "base_columns = [\n",
    "    # 全是离散特征\n",
    "    education, marital_status, relationship, workclass, occupation,\n",
    "    age_buckets,\n",
    "]\n",
    "\n",
    "crossed_columns = [\n",
    "    tf.feature_column.crossed_column(\n",
    "        ['education', 'occupation'], hash_bucket_size=1000),\n",
    "    tf.feature_column.crossed_column(\n",
    "        [age_buckets, 'education', 'occupation'], hash_bucket_size=1000\n",
    "    )\n",
    "]\n",
    "\n",
    "# 3. The Deep Model: Neural Network with Embeddings\n",
    "\"\"\"\n",
    "1. Sparse Features -> Embedding vector -> 串联(Embedding vector, 连续特征) -> 输入到Hidden Layer\n",
    "2. Embedding Values随机初始化\n",
    "3. 另外一种处理离散特征的方法是：one-hot or multi-hot representation. 但是仅仅适用于维度较低的，embedding是更加通用的做法\n",
    "4. embedding_column(embedding);indicator_column(multi-hot);\n",
    "\"\"\"\n",
    "deep_columns = [\n",
    "    age,\n",
    "    education_num,\n",
    "    capital_gain,\n",
    "    capital_loss,\n",
    "    hours_per_week,\n",
    "    tf.feature_column.indicator_column(workclass),\n",
    "    tf.feature_column.indicator_column(education),\n",
    "    tf.feature_column.indicator_column(marital_status),\n",
    "    tf.feature_column.indicator_column(relationship),\n",
    "\n",
    "    # To show an example of embedding\n",
    "    tf.feature_column.embedding_column(occupation, dimension=8)\n",
    "]\n",
    "\n",
    "model_dir = './model/wide_deep'\n",
    "\n",
    "# 4. Combine Wide & Deep\n",
    "model = tf.estimator.DNNLinearCombinedClassifier(\n",
    "    model_dir = model_dir,\n",
    "    linear_feature_columns=base_columns + crossed_columns,\n",
    "    dnn_feature_columns=deep_columns,\n",
    "    dnn_hidden_units=[100,50]\n",
    ")\n",
    "\n",
    "# 5. Train & Evaluate\n",
    "_CSV_COLUMNS = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "    'marital_status', 'occupation', 'relationship', 'race', 'gender',\n",
    "    'capital_gain', 'capital_loss', 'hours_per_week', 'native_country',\n",
    "    'income_bracket'\n",
    "]\n",
    "_CSV_COLUMN_DEFAULTS = [[0], [''], [0], [''], [0], [''], [''], [''], [''], [''],\n",
    "                        [0], [0], [0], [''], ['']]\n",
    "_NUM_EXAMPLES = {\n",
    "    'train': 32561,\n",
    "    'validation': 16281,\n",
    "}\n",
    "\n",
    "def input_fn(data_file, num_epochs, shuffle, batch_size):\n",
    "    \"\"\"为Estimator创建一个input function\"\"\"\n",
    "    assert tf.gfile.Exists(data_file), \"{0} not found.\".format(data_file)\n",
    "\n",
    "    def parse_csv(line):\n",
    "        print(\"Parsing\", data_file)\n",
    "        # tf.decode_csv会把csv文件转换成很a list of Tensor,一列一个。record_defaults用于指明每一列的缺失值用什么填充\n",
    "        columns = tf.decode_csv(line, record_defaults=_CSV_COLUMN_DEFAULTS)\n",
    "        features = dict(zip(_CSV_COLUMNS, columns))\n",
    "        labels = features.pop('income_bracket')\n",
    "        return features, tf.equal(labels, '>50K') # tf.equal(x, y) 返回一个bool类型Tensor， 表示x == y, element-wise\n",
    "\n",
    "    dataset = tf.data.TextLineDataset(data_file) \\\n",
    "                .map(parse_csv, num_parallel_calls=5)\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=_NUM_EXAMPLES['train'] + _NUM_EXAMPLES['validation'])\n",
    "\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    batch_features, batch_labels = iterator.get_next()\n",
    "    return batch_features, batch_labels\n",
    "\n",
    "# Train + Eval\n",
    "train_epochs = 6\n",
    "epochs_per_eval = 2\n",
    "batch_size = 40\n",
    "train_file = './data/adult.data'\n",
    "test_file  = './data/adult.test'\n",
    "\n",
    "for n in range(train_epochs // epochs_per_eval):\n",
    "    model.train(input_fn=lambda: input_fn(train_file, epochs_per_eval, True, batch_size))\n",
    "    results = model.evaluate(input_fn=lambda: input_fn(\n",
    "        test_file, 1, False, batch_size))\n",
    "\n",
    "    # Display Eval results\n",
    "    print(\"Results at epoch {0}\".format((n+1) * epochs_per_eval))\n",
    "    print('-'*30)\n",
    "\n",
    "    for key in sorted(results):\n",
    "        print(\"{0:20}: {1:.4f}\".format(key, results[key]))\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
