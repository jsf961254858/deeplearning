{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0107 20:57:51.844187 30564 estimator.py:209] Using config: {'_model_dir': './ckpt', '_tf_random_seed': 2019, '_save_summary_steps': 100, '_save_checkpoints_steps': 100, '_save_checkpoints_secs': None, '_session_config': device_count {\n",
      "  key: \"CPU\"\n",
      "  value: 1\n",
      "}\n",
      "gpu_options {\n",
      "  allow_growth: true\n",
      "}\n",
      "allow_soft_placement: true\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001B403FF5C18>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "I0107 20:57:51.845165 30564 estimator_training.py:186] Not using Distribute Coordinator.\n",
      "I0107 20:57:51.846174 30564 training.py:612] Running training and evaluation locally (non-distributed).\n",
      "I0107 20:57:51.847159 30564 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 100 or save_checkpoints_secs None.\n",
      "I0107 20:57:51.857146 30564 estimator.py:360] Skipping training since max_steps has already saved.\n",
      "I0107 20:57:51.868129 30564 estimator.py:1145] Calling model_fn.\n",
      "I0107 20:57:52.331873 30564 estimator.py:1147] Done calling model_fn.\n",
      "I0107 20:57:52.332851 30564 export_utils.py:170] Signatures INCLUDED in export for Classify: None\n",
      "I0107 20:57:52.333848 30564 export_utils.py:170] Signatures INCLUDED in export for Regress: None\n",
      "I0107 20:57:52.333848 30564 export_utils.py:170] Signatures INCLUDED in export for Predict: ['prediction', 'serving_default']\n",
      "I0107 20:57:52.333848 30564 export_utils.py:170] Signatures INCLUDED in export for Train: None\n",
      "I0107 20:57:52.334877 30564 export_utils.py:170] Signatures INCLUDED in export for Eval: None\n",
      "I0107 20:57:52.368754 30564 saver.py:1280] Restoring parameters from ./ckpt\\model.ckpt-1600\n",
      "I0107 20:57:52.394684 30564 builder_impl.py:661] Assets added to graph.\n",
      "I0107 20:57:52.395709 30564 builder_impl.py:456] No assets to write.\n",
      "I0107 20:57:53.099707 30564 builder_impl.py:421] SavedModel written to: ./model\\temp-b'1578401871'\\saved_model.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['global_step', 'item_embedding', 'layer_1/bias', 'layer_1/kernel', 'layer_2/bias', 'layer_2/kernel', 'user_vector/bias', 'user_vector/kernel']\n",
      "[ 0.01600168  0.02402179 -0.01918003 -0.01017302 -0.02798696  0.0081965\n",
      " -0.00508118 -0.00785436  0.00856501  0.01278291  0.01015852 -0.00397919\n",
      " -0.0016386  -0.02894066  0.00048572  0.02356471]\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "\n",
    "def parse_fn(example):\n",
    "    example_fmt = {\n",
    "        \"visit_items_index\": tf.FixedLenFeature([5], tf.int64),\n",
    "        \"continuous_features_value\": tf.FixedLenFeature([16], tf.float32),\n",
    "        \"next_visit_item_index\": tf.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    parsed = tf.parse_single_example(example, example_fmt)\n",
    "    next_visit_item_index = parsed.pop(\"next_visit_item_index\")\n",
    "    return parsed, next_visit_item_index\n",
    "\n",
    "def input_fn(path, parallel_num, epoch_num, batch_size):\n",
    "    files = tf.data.Dataset.list_files(path, shuffle=True)\n",
    "    data_set = files.apply(\n",
    "        tf.contrib.data.parallel_interleave(\n",
    "            map_func=lambda filename: tf.data.TFRecordDataset(filename),\n",
    "            cycle_length=parallel_num))\n",
    "    data_set = data_set.repeat(epoch_num)\n",
    "    data_set = data_set.map(map_func=parse_fn, num_parallel_calls=parallel_num)\n",
    "    data_set = data_set.prefetch(buffer_size=256)\n",
    "    data_set = data_set.batch(batch_size=batch_size)\n",
    "    return data_set\n",
    "\n",
    "def model_fn(features, labels, mode, params, config):\n",
    "\n",
    "    visit_items_index = features[\"visit_items_index\"]    # num * 5\n",
    "    continuous_features_value = features[\"continuous_features_value\"]  # num * 16\n",
    "    next_visit_item_index = labels    # num\n",
    "    keep_prob = params[\"keep_prob\"]\n",
    "    embedding_size = params[\"embedding_size\"]\n",
    "    item_num = params[\"item_num\"]\n",
    "    learning_rate = params[\"learning_rate\"]\n",
    "    top_k = params[\"top_k\"]\n",
    "\n",
    "    # items embedding 初始化\n",
    "    initializer = tf.initializers.random_uniform(minval=-0.5 / embedding_size, maxval=0.5 / embedding_size)\n",
    "    partitioner = tf.fixed_size_partitioner(num_shards=embedding_size)\n",
    "    item_embedding = tf.get_variable(\"item_embedding\", [item_num, embedding_size],\n",
    "                                     tf.float32, initializer=initializer, partitioner=partitioner)\n",
    "\n",
    "    visit_items_embedding = tf.nn.embedding_lookup(item_embedding, visit_items_index)       # num * 5 * embedding_size\n",
    "    visit_items_average_embedding = tf.reduce_mean(visit_items_embedding, axis=1)     # num * embedding_size\n",
    "    input_embedding = tf.concat([visit_items_average_embedding, continuous_features_value], 1)   # num * (embedding_size + 16)\n",
    "    kernel_initializer_1 = tf.initializers.random_normal(mean=0.0, stddev=0.1)\n",
    "    bias_initializer_1 = tf.initializers.random_normal(mean=0.0, stddev=0.1)\n",
    "    layer_1 = tf.layers.dense(input_embedding, 64, activation=tf.nn.relu,\n",
    "                              kernel_initializer=kernel_initializer_1,\n",
    "                              bias_initializer=bias_initializer_1, name=\"layer_1\")\n",
    "    layer_dropout_1 = tf.nn.dropout(layer_1, keep_prob=keep_prob, name=\"layer_dropout_1\")\n",
    "    kernel_initializer_2 = tf.initializers.random_normal(mean=0.0, stddev=0.1)\n",
    "    bias_initializer_2 = tf.initializers.random_normal(mean=0.0, stddev=0.1)\n",
    "    layer_2 = tf.layers.dense(layer_dropout_1, 32, activation=tf.nn.relu,\n",
    "                              kernel_initializer=kernel_initializer_2,\n",
    "                              bias_initializer=bias_initializer_2, name=\"layer_2\")\n",
    "    layer_dropout_2 = tf.nn.dropout(layer_2, keep_prob=keep_prob, name=\"layer_dropout_2\")\n",
    "    # user vector, num * embedding_size\n",
    "    kernel_initializer_3 = tf.initializers.random_normal(mean=0.0, stddev=0.1)\n",
    "    bias_initializer_3 = tf.initializers.random_normal(mean=0.0, stddev=0.1)\n",
    "    user_vector = tf.layers.dense(layer_dropout_2, embedding_size, activation=tf.nn.relu,\n",
    "                                  kernel_initializer=kernel_initializer_3,\n",
    "                                  bias_initializer=bias_initializer_3, name=\"user_vector\")\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        # 训练\n",
    "        output_embedding = tf.nn.embedding_lookup(item_embedding, next_visit_item_index)  # num * embedding_size\n",
    "        logits = tf.matmul(user_vector, output_embedding, transpose_a=False, transpose_b=True)  # num * num\n",
    "        yhat = tf.nn.softmax(logits)  # num * num\n",
    "        cross_entropy = tf.reduce_mean(-tf.log(tf.matrix_diag_part(yhat) + 1e-16))\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        train = optimizer.minimize(cross_entropy, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=cross_entropy, train_op=train)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        # 评估\n",
    "        output_embedding = tf.nn.embedding_lookup(item_embedding, next_visit_item_index)  # num * embedding_size\n",
    "        logits = tf.matmul(user_vector, output_embedding, transpose_a=False, transpose_b=True)  # num * num\n",
    "        yhat = tf.nn.softmax(logits)  # num * num\n",
    "        cross_entropy = tf.reduce_mean(-tf.log(tf.matrix_diag_part(yhat) + 1e-16))\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=cross_entropy)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "\n",
    "        logits_predict = tf.matmul(user_vector, item_embedding, transpose_a=False, transpose_b=True)  # num *  item_num\n",
    "        yhat_predict = tf.nn.softmax(logits_predict)  # num *  item_num\n",
    "        # _, indices = tf.nn.top_k(yhat_predict, k=top_k, sorted=True)\n",
    "        # index = tf.identity(indices, name=\"index\")  # num * top_k\n",
    "        # 预测\n",
    "        predictions = {\n",
    "            \"user_vector\": user_vector\n",
    "            #\"index\": index\n",
    "        }\n",
    "        export_outputs = {\n",
    "            \"prediction\": tf.estimator.export.PredictOutput(predictions)\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions, export_outputs=export_outputs)\n",
    "\n",
    "def build_estimator():\n",
    "    params = {\"keep_prob\": 0.5, \"embedding_size\": 16, \"item_num\": 500, \"learning_rate\": 0.05, \"top_k\": 2}\n",
    "    session_config = tf.ConfigProto(device_count={\"CPU\": 1}, allow_soft_placement=True, log_device_placement=False)\n",
    "    session_config.gpu_options.allow_growth = True\n",
    "    config = tf.estimator.RunConfig(\n",
    "        model_dir=\"./ckpt\",\n",
    "        tf_random_seed=2019,\n",
    "        save_checkpoints_steps=100,\n",
    "        session_config=session_config,\n",
    "        keep_checkpoint_max=5,\n",
    "        log_step_count_steps=100\n",
    "    )\n",
    "    estimator = tf.estimator.Estimator(model_fn=model_fn, config=config, params=params)\n",
    "    return estimator\n",
    "\n",
    "def set_dist_env():\n",
    "    if FLAGS.is_distributed:\n",
    "        ps_hosts = FLAGS.strps_hosts.split(\",\")\n",
    "        worker_hosts = FLAGS.strwork_hosts.split(\",\")\n",
    "        job_name = FLAGS.job_name\n",
    "        task_index = FLAGS.task_index\n",
    "        chief_hosts = worker_hosts[0:1]  # get first worker as chief\n",
    "        worker_hosts = worker_hosts[2:]  # the rest as worker\n",
    "\n",
    "        # use #worker=0 as chief\n",
    "        if job_name == \"worker\" and task_index == 0:\n",
    "            job_name = \"chief\"\n",
    "        # use #worker=1 as evaluator\n",
    "        if job_name == \"worker\" and task_index == 1:\n",
    "            job_name = 'evaluator'\n",
    "            task_index = 0\n",
    "        # the others as worker\n",
    "        if job_name == \"worker\" and task_index > 1:\n",
    "            task_index -= 2\n",
    "\n",
    "        tf_config = {'cluster': {'chief': chief_hosts, 'worker': worker_hosts, 'ps': ps_hosts},\n",
    "                     'task': {'type': job_name, 'index': task_index}}\n",
    "        os.environ['TF_CONFIG'] = json.dumps(tf_config)\n",
    "\n",
    "def train_eval_save():\n",
    "\n",
    "    set_dist_env()\n",
    "\n",
    "    estimator = build_estimator()\n",
    "\n",
    "    # 训练\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn=lambda: input_fn(\n",
    "            path=\"./data/train.tfrecords\",\n",
    "            parallel_num=32,\n",
    "            epoch_num=11,\n",
    "            batch_size=32),\n",
    "        max_steps=1600\n",
    "    )\n",
    "    # 评估\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn=lambda: input_fn(\n",
    "            path=\"./data/evaluation.tfrecords\",\n",
    "            parallel_num=32,\n",
    "            epoch_num=1,\n",
    "            batch_size=32),\n",
    "        steps=15,     # 验证集评估多少批数据\n",
    "        start_delay_secs=1,    # 在多少秒后开始评估\n",
    "        throttle_secs=20  # evaluate every 20seconds\n",
    "    )\n",
    "    # 训练和评估\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "\n",
    "    # 模型保存\n",
    "    features_spec = {\n",
    "        \"visit_items_index\": tf.placeholder(tf.int64, shape=[None, 5], name=\"visit_items_index\"),\n",
    "        \"continuous_features_value\": tf.placeholder(tf.float32, shape=[None, 16], name=\"continuous_features_value\")\n",
    "    }\n",
    "    serving_input_receiver_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(features_spec)\n",
    "    estimator.export_savedmodel(\n",
    "        \"./model\",\n",
    "        serving_input_receiver_fn)\n",
    "    print(estimator.get_variable_names())\n",
    "    item_emb = estimator.get_variable_value('item_embedding')\n",
    "    print(item_emb[0])\n",
    "    #for emb in item_emb:\n",
    "    #    print(emb)\n",
    "    # 将item_emb 写入文件\n",
    "    with open('data.txt','w') as f:\n",
    "        for vec in item_emb:\n",
    "            vec_str = \" \".join([str(_) for _ in vec])\n",
    "            f.write(vec_str + \"\\n\")\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    train_eval_save()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "    tf.app.run(main=main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
