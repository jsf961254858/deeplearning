{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_fn(example):\n",
    "    \n",
    "    f = {\n",
    "        \"visit_items_index\":tf.FixedLenFeature([5], tf.int64),\n",
    "        \"continuous_features_value\":tf.FixedLenFeature([16],tf.float32),\n",
    "        \"next_visit_item_index\":tf.FixedLenFeature([],tf.int64)\n",
    "    }\n",
    "    parsed = tf.parse_single_example(example,f)\n",
    "    next_visit_item_index = parsed.pop(\"next_visit_item_index\")\n",
    "    return parsed, next_visit_item_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(path, parallel_num, epoch_num, batch_size):\n",
    "    \n",
    "    files = tf.data.Dataset.list_files(path, shuffle=True)\n",
    "    dataset = files.apply(\n",
    "        tf.contrib.data.parallel_interleave(\n",
    "            lambda filename: tf.data.TFRecordDataset(filename),\n",
    "            cycle_length=parallel_num\n",
    "    ))   \n",
    "    dataset = dataset.repeat(epoch_num).map(map_func=parse_fn, num_parallel_calls=parallel_num)\n",
    "    dataset = dataset.prefetch(10*batch_size).batch(batch_size)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    features , labels = iterator.get_next()\n",
    "    return features, labels\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params, config):\n",
    "    \n",
    "    visit_items_index = features['visit_items_index']  # num * 5\n",
    "    continuous_features_value = features['continuous_features_value'] # num * 16\n",
    "    next_visit_item_index = labels\n",
    "    keep_prob = params[\"keep_prob\"]\n",
    "    embedding_size = params[\"embedding_size\"]\n",
    "    item_num = params[\"item_num\"]\n",
    "    learning_rate = params[\"learning_rate\"]\n",
    "    top_k = params[\"top_k\"]\n",
    "    deep_layers = params[\"deep_layers\"]\n",
    "    \n",
    "    \n",
    "    # items embedding init\n",
    "    initializer = tf.initializers.random_uniform(minval=-0.5/embedding_size, maxval=0.5/embedding_size)\n",
    "    partitioner = tf.fixed_size_partitioner(num_shards=embedding_size)\n",
    "    item_embedding = tf.get_variable(\"item_embedding\",\n",
    "                                     [item_num, embedding_size],\n",
    "                                    tf.float32,\n",
    "                                    initializer=initializer,\n",
    "                                    partitioner=partitioner)\n",
    "    visit_items_embedding = tf.nn.embedding_lookup(item_embedding, visit_items_index) # num * 5 * embedding_size\n",
    "    visit_items_average_embedding = tf.reduce_mean(visit_items_embedding, axis=1)  # num * embedding_size\n",
    "    inputs = tf.concat([visit_items_average_embedding, continuous_features_value], 1)  # num * (embedding_size + 16)\n",
    "    \n",
    "    # dnn model\n",
    "    for i in range(len(deep_layers)):\n",
    "        inputs = tf.contrib.layers.fully_connected(inputs=inputs, \n",
    "                                            num_outputs=deep_layers[i],\n",
    "                                            weights_initializer=tf.initializers.random_normal(mean=0.0, stddev=0.1),\n",
    "                                            biases_initializer=tf.initializers.random_normal(mean=0.0, stddev=0.1))\n",
    "        inputs = tf.nn.dropout(inputs, keep_prob=keep_prob, name='layer_%d' % i)\n",
    "    \n",
    "    # user vector 是最后一层的output\n",
    "    user_vector = inputs\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        \n",
    "        output_embedding = tf.nn.embedding_lookup(item_embedding, next_visit_item_index) # num * embedding_size\n",
    "        logits = tf.matmul(user_vector, output_embedding, transpose_a=False, transpose_b=True)    # num * num\n",
    "        yhat = tf.nn.softmax(logits)  # num * num\n",
    "        cross_entropy = tf.reduce_mean(-tf.log(tf.matrix_diag_part(yhat) + 1e-16))\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        train = optimizer.minimize(cross_entropy, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=cross_entropy, train_op=train)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        \n",
    "        output_embedding = tf.nn.embedding_lookup(item_embedding, next_visit_item_index)\n",
    "        logits = tf.matmul(user_vector, output_embedding, transpose_a=False, transpose_b=True)\n",
    "        yhat = tf.nn.softmax(logits)\n",
    "        cross_entropy = tf.reduce_mean(-tf.log(tf.matrix_diag_part(yhat)+1e-16))\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=cross_entropy)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        \n",
    "        logits_predict = tf.matmul(user_vector, item_embedding, transpose_a=False, transpose_b=True)\n",
    "        yhat_predict = tf.nn.softmax(logits_predict)\n",
    "        _, indices = tf.nn.top_k(yhat_predict, k=top_k, sorted=True)\n",
    "        index = tf.identity(indices, name=\"index\")\n",
    "        \n",
    "        predictions = {\n",
    "            \"user_vector\":user_vector,\n",
    "            \"index\":index,\n",
    "            \"item_embedding\":item_embedding\n",
    "        }\n",
    "        export_outputs = {\n",
    "            \"prediction\":tf.estimator.export.PredictOutput(predictions)\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions, export_outputs=export_outputs)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_estimator():\n",
    "    params = {\n",
    "        \"keep_prob\":0.5,\n",
    "        \"embedding_size\":16,\n",
    "        \"item_num\":500,\n",
    "        \"learning_rate\":0.05,\n",
    "        \"top_k\":2,\n",
    "        \"deep_layers\":[64,32,16]\n",
    "    }\n",
    "    config = tf.estimator.RunConfig(\n",
    "        model_dir=\"./ckpt\",\n",
    "        tf_random_seed=2019,\n",
    "        save_checkpoints_steps=100,\n",
    "        keep_checkpoint_max=5,\n",
    "        log_step_count_steps=100\n",
    "    )\n",
    "    estimator = tf.estimator.Estimator(model_fn=model_fn, config=config, params=params)\n",
    "    return estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0106 20:31:31.052049 32168 deprecation.py:323] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "W0106 20:31:31.074018 32168 deprecation.py:323] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\util\\random_seed.py:58: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0106 20:31:33.407733 32168 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0106 20:31:33.408739 32168 deprecation.py:323] From <ipython-input-4-e73e5f1f0bd7>:7: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.parallel_interleave(...)`.\n",
      "W0106 20:31:33.408739 32168 deprecation.py:323] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\data\\python\\ops\\interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
      "W0106 20:31:33.437652 32168 deprecation.py:323] From <ipython-input-4-e73e5f1f0bd7>:11: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "W0106 20:31:33.754797 32168 deprecation.py:506] From <ipython-input-5-0e13c87e90db>:32: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0106 20:31:34.893926 32168 deprecation.py:323] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "W0106 20:31:34.952799 32168 deprecation.py:323] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1066: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file utilities to get mtimes.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    estimator = build_estimator()\n",
    "    \n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn=lambda:input_fn(\n",
    "            path='./data/train.tf.records',\n",
    "            parallel_num=5,\n",
    "            epoch_num = 10,\n",
    "            batch_size=32),\n",
    "        max_steps=1600)\n",
    "    \n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn=lambda:input_fn(\n",
    "            path='./data/train.tf.records',\n",
    "            parallel_num=5,\n",
    "            epoch_num = 1,\n",
    "            batch_size=32),\n",
    "        steps=15,\n",
    "        start_delay_secs=1,\n",
    "        throttle_secs=20\n",
    "    )\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "    \n",
    "    features_spec = {\n",
    "        \"visit_items_index\": tf.placeholder(tf.int64, shape=[None, 5], name=\"visit_items_index\"),\n",
    "        \"continuous_features_value\":tf.placeholder(tf.float32, shape=[None, 16], name=\"continuous_features_value\")\n",
    "    }\n",
    "    serving_input_receiver_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(features_spec)\n",
    "    estimator.export_savedmodel(\"./model\", serving_input_receiver_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
